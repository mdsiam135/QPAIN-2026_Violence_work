{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14325391,"sourceType":"datasetVersion","datasetId":9119194},{"sourceId":14337539,"sourceType":"datasetVersion","datasetId":9154024}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.utils.data import Dataset\nimport os\nimport shutil\nimport transformers\nfrom packaging import version\n\n# Configuration\nBANGLABERT_NAME = \"csebuetnlp/banglabert\"\nXLM_ROBERTA_NAME = \"xlm-roberta-base\"\nMAX_LEN = 256 # Restored for better accuracy\nBATCH_SIZE = 16\nEPOCHS = 5\nLEARNING_RATE = 2e-5\n\nclass MultiModelDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer1, tokenizer2, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer1 = tokenizer1\n        self.tokenizer2 = tokenizer2\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        label = self.labels[item]\n\n        encoding1 = self.tokenizer1.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        encoding2 = self.tokenizer2.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'input_ids1': encoding1['input_ids'].flatten(),\n            'attention_mask1': encoding1['attention_mask'].flatten(),\n            'input_ids2': encoding2['input_ids'].flatten(),\n            'attention_mask2': encoding2['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\nclass FeatureEnsembleModel(nn.Module):\n    def __init__(self, model1_name, model2_name, num_labels, class_weights=None):\n        super(FeatureEnsembleModel, self).__init__()\n        self.model1 = AutoModel.from_pretrained(model1_name)\n        self.model2 = AutoModel.from_pretrained(model2_name)\n        \n        hidden_size1 = self.model1.config.hidden_size\n        hidden_size2 = self.model2.config.hidden_size\n        \n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(hidden_size1 + hidden_size2, num_labels)\n        self.num_labels = num_labels\n        self.class_weights = class_weights\n\n    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, labels=None):\n        outputs1 = self.model1(input_ids=input_ids1, attention_mask=attention_mask1)\n        features1 = outputs1.last_hidden_state[:, 0, :] \n        \n        outputs2 = self.model2(input_ids=input_ids2, attention_mask=attention_mask2)\n        features2 = outputs2.last_hidden_state[:, 0, :]\n        \n        combined_features = torch.cat((features1, features2), dim=1)\n        combined_features = self.dropout(combined_features)\n        \n        logits = self.classifier(combined_features)\n        \n        loss = None\n        if labels is not None:\n            if self.class_weights is not None:\n                loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n            else:\n                loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            \n        return (loss, logits) if loss is not None else logits\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions, average='macro')\n    precision = precision_score(labels, predictions, average='macro', zero_division=0)\n    recall = recall_score(labels, predictions, average='macro', zero_division=0)\n    return {\n        'accuracy': acc,\n        'f1_macro': f1,\n        'precision_macro': precision,\n        'recall_macro': recall\n    }\n\ndef main():\n    # --- DATA PATHS ---\n    train_path = '/kaggle/input/violence-dataset-2-0/train - Sheet1.csv'\n    val_path = '/kaggle/input/violence-dataset-2-0/Validation - Sheet1.csv' \n    test_path = '/kaggle/input/violence-dataset-2-0/test.csv'\n    \n    # Load data\n    train_df = pd.read_csv(train_path)\n    val_df = pd.read_csv(val_path)\n    test_df = pd.read_csv(test_path)\n    \n    print(f\"Loaded {len(train_df)} train, {len(val_df)} validation, and {len(test_df)} test samples.\")\n\n    num_labels = 3 \n    \n    # Compute class weights to handle imbalance\n    weights = compute_class_weight('balanced', classes=np.unique(train_df['label']), y=train_df['label'])\n    class_weights = torch.tensor(weights, dtype=torch.float)\n\n    tokenizer1 = AutoTokenizer.from_pretrained(BANGLABERT_NAME)\n    tokenizer2 = AutoTokenizer.from_pretrained(XLM_ROBERTA_NAME)\n\n    train_dataset = MultiModelDataset(train_df.text.to_numpy(), train_df.label.to_numpy(), tokenizer1, tokenizer2, MAX_LEN)\n    val_dataset = MultiModelDataset(val_df.text.to_numpy(), val_df.label.to_numpy(), tokenizer1, tokenizer2, MAX_LEN)\n    test_dataset = MultiModelDataset(test_df.text.to_numpy(), test_df.label.to_numpy(), tokenizer1, tokenizer2, MAX_LEN)\n\n    model = FeatureEnsembleModel(BANGLABERT_NAME, XLM_ROBERTA_NAME, num_labels, class_weights=class_weights)\n\n    # Handle version compatibility\n    eval_strat_key = \"eval_strategy\" if version.parse(transformers.__version__) >= version.parse(\"4.41.0\") else \"evaluation_strategy\"\n\n    training_args = TrainingArguments(\n        output_dir='./ensemble_results',\n        num_train_epochs=EPOCHS,\n        per_device_train_batch_size=BATCH_SIZE,\n        per_device_eval_batch_size=BATCH_SIZE,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir='./logs',\n        logging_steps=10,\n        **{eval_strat_key: \"epoch\"},\n        save_strategy=\"no\",\n        report_to=\"none\"\n    )\n\n    class EnsembleTrainer(Trainer):\n        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n            labels = inputs.get(\"labels\")\n            outputs = model(\n                input_ids1=inputs.get(\"input_ids1\"),\n                attention_mask1=inputs.get(\"attention_mask1\"),\n                input_ids2=inputs.get(\"input_ids2\"),\n                attention_mask2=inputs.get(\"attention_mask2\"),\n                labels=labels\n            )\n            loss = outputs[0] if isinstance(outputs, tuple) else outputs\n            return (loss, outputs) if return_outputs else loss\n\n    trainer = EnsembleTrainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset, compute_metrics=compute_metrics)\n\n    print(\"Starting training...\")\n    trainer.train()\n\n    print(\"Evaluating on test set...\")\n    test_results = trainer.evaluate(test_dataset)\n    print(f\"Test Results: {test_results}\")\n\n    # --- DISK CLEANUP BEFORE SAVE ---\n    print(\"Cleaning up temporary files to free disk space...\")\n    if os.path.exists('./ensemble_results'):\n        shutil.rmtree('./ensemble_results')\n    if os.path.exists('./logs'):\n        shutil.rmtree('./logs')\n\n    # Save ONLY the final model weights\n    print(\"Saving final model weights...\")\n    model.to('cpu') # Move to CPU to avoid memory spikes\n    torch.save(model.state_dict(), \"feature_ensemble_model.pt\")\n    print(\"Success! Model saved to feature_ensemble_model.pt\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T00:58:46.166449Z","iopub.execute_input":"2025-12-30T00:58:46.166911Z","iopub.status.idle":"2025-12-30T01:39:44.169379Z","shell.execute_reply.started":"2025-12-30T00:58:46.166869Z","shell.execute_reply":"2025-12-30T01:39:44.168525Z"}},"outputs":[{"name":"stderr","text":"2025-12-30 00:59:03.530628: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767056343.712459      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767056343.763559      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Loaded 8339 train, 1790 validation, and 1790 test samples.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff8bb13891d348788c1162c1948d8a57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/586 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4aeb4f6a22c49009e431e17db997c3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14414272c5f14523afe8b49ab947fa84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0192433e8ce40ef80e737ff2205b911"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0f5299064eb408197472eb5c8c5483d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"587c87a654354562869627d4853c64cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cccf60422e6465da58fa35b2e9dabca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46deb9be17d547a8a80f86948c74ae40"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfbd2e21d3894913b17bf176f9d781ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69af5c9e34fa46f087a96b0acddc8146"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9f7287b43244210a2afbd7d7c32bf6d"}},"metadata":{}},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2610' max='2610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2610/2610 39:32, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1 Macro</th>\n      <th>Precision Macro</th>\n      <th>Recall Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.520200</td>\n      <td>1.150099</td>\n      <td>0.611732</td>\n      <td>0.613348</td>\n      <td>0.663882</td>\n      <td>0.627820</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.408800</td>\n      <td>1.425340</td>\n      <td>0.637989</td>\n      <td>0.637718</td>\n      <td>0.670373</td>\n      <td>0.649994</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.087300</td>\n      <td>1.594532</td>\n      <td>0.672067</td>\n      <td>0.672377</td>\n      <td>0.688309</td>\n      <td>0.680799</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.054600</td>\n      <td>2.101224</td>\n      <td>0.663687</td>\n      <td>0.661967</td>\n      <td>0.685702</td>\n      <td>0.671585</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.002800</td>\n      <td>2.346117</td>\n      <td>0.660894</td>\n      <td>0.658744</td>\n      <td>0.687296</td>\n      <td>0.669825</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Evaluating on test set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='112' max='112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [112/112 00:26]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Test Results: {'eval_loss': 1.3967911005020142, 'eval_accuracy': 0.8575418994413407, 'eval_f1_macro': 0.8404588055584856, 'eval_precision_macro': 0.8793587256501164, 'eval_recall_macro': 0.82668849754921, 'eval_runtime': 26.8717, 'eval_samples_per_second': 66.613, 'eval_steps_per_second': 4.168, 'epoch': 5.0}\nCleaning up temporary files to free disk space...\nSaving final model weights...\nSuccess! Model saved to feature_ensemble_model.pt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}